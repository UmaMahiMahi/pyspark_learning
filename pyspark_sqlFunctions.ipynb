{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be866cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[1]\") \\\n",
    "     .appName(\"SparkByExamples.com\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb312343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015a3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e53ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Absenteeism_new_data.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f26856",
   "metadata": {},
   "source": [
    "# PySpark Aggregate Functions with Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d8921",
   "metadata": {},
   "source": [
    "approx_count_distinct----- unique items count in a specific column\n",
    "\n",
    "avg- avg of specific column\n",
    "\n",
    "collect_list-- list of all items in a column\n",
    "\n",
    "collect_set- list of unique items in a column\n",
    "\n",
    "countDistinct- distinct items count\n",
    "\n",
    "count- total count\n",
    "\n",
    "grouping\n",
    "\n",
    "first\n",
    "\n",
    "last\n",
    "\n",
    "kurtosis\n",
    "\n",
    "max\n",
    "\n",
    "min\n",
    "\n",
    "mean\n",
    "\n",
    "skewness\n",
    "\n",
    "stddev\n",
    "\n",
    "stddev_samp\n",
    "\n",
    "stddev_pop\n",
    "\n",
    "sum\n",
    "\n",
    "sumDistinct\n",
    "\n",
    "variance, var_samp, var_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f24013",
   "metadata": {},
   "source": [
    "# approx_count_distinct Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b3ede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(approx_count_distinct(pets)=6)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(approx_count_distinct(df[\"pets\"])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f36571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(approx_count_distinct(df[\"pets\"])).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df503f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "****************************************************************************************************\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|collect_list(Pets)                                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0, 4, 0, 0, 0, 2, 0, 0, 0, 0, 4, 0, 8, 0, 5, 0, 0, 0, 1, 0, 0, 8, 0, 0, 8, 0, 8, 1, 0, 1, 2, 0, 4, 2, 0, 1, 1, 8, 2, 1]|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "None\n",
      "[0, 1, 5, 2, 4, 8]\n"
     ]
    }
   ],
   "source": [
    "#approx_count_distinct- returns \"approx unique value\"\n",
    "print(df.select(approx_count_distinct(df[\"Pets\"])).collect()[0][0])\n",
    "\n",
    "print(\"*\"*100)\n",
    "\n",
    "# avg\n",
    "df.select(avg(\"Pets\")).collect()[0][0]\n",
    "\n",
    "#collect_list  collect all items in list including duplicates \n",
    "\n",
    "print(df.select(collect_list(\"Pets\")).show(truncate=False))\n",
    "\n",
    "#collect_set - unique items\n",
    "\n",
    "\n",
    "print(df.select(collect_set(\"Pets\")).collect()[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2dea0",
   "metadata": {},
   "source": [
    "# [PySpark Window Functions](https://sparkbyexamples.com/pyspark/pyspark-window-functions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c9fe1",
   "metadata": {},
   "source": [
    "row_number()\n",
    "rank()\n",
    "percent_rank()\n",
    "dense_rank(): \n",
    "ntile(n: Int): \n",
    "cume_dist(): \n",
    "lag(columnName: String, offset: Int): \n",
    "lag(columnName: String, offset: Int, defaultValue: Any): \n",
    "lead(columnName: String, offset: Int): Column\n",
    "lead(columnName: String, offset: Int): Column\n",
    "lead(columnName: String, offset: Int, defaultValue: Any): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a5ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588c6d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+\n",
      "|Pets|Education|row_number|\n",
      "+----+---------+----------+\n",
      "|0   |1        |1         |\n",
      "|0   |1        |2         |\n",
      "|0   |1        |3         |\n",
      "|0   |1        |4         |\n",
      "|0   |1        |5         |\n",
      "|0   |1        |6         |\n",
      "|0   |1        |7         |\n",
      "|0   |1        |8         |\n",
      "|0   |1        |9         |\n",
      "|0   |1        |10        |\n",
      "|0   |1        |11        |\n",
      "|0   |1        |12        |\n",
      "|0   |1        |13        |\n",
      "|0   |1        |14        |\n",
      "|0   |2        |15        |\n",
      "|0   |2        |16        |\n",
      "|0   |3        |17        |\n",
      "|0   |3        |18        |\n",
      "|0   |3        |19        |\n",
      "|0   |3        |20        |\n",
      "+----+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"Pets\").orderBy(\"Education\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .select([\"Pets\",\"Education\",\"row_number\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27123246",
   "metadata": {},
   "source": [
    "# PySpark SQL Date and Timestamp Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0ce67",
   "metadata": {},
   "source": [
    "# # pyspark sql date functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fe60a",
   "metadata": {},
   "source": [
    "current_date()\t---- \n",
    "\n",
    "current date as a date column\n",
    "\n",
    "******************************************************************************************\n",
    "date_format(dateExpr,format)---\n",
    "\n",
    "Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.\n",
    "\n",
    "********************************************************************************************\n",
    "to_date()\t----Converts the column into `DateType` by casting rules to `DateType`.\n",
    "\n",
    "*********************************************************************************\n",
    "to_date(column, fmt)\n",
    "--Converts the column into a `DateType` with a specified format\n",
    "*************************************************************************************************\n",
    "\n",
    "\n",
    "add_months(Column, numMonths)--\n",
    "\n",
    "Returns the date that is `numMonths` after `startDate`\n",
    "\n",
    "*********************************************************************************************\n",
    "date_add(column, days)\n",
    "\n",
    "********************************************************************************************\n",
    "\n",
    "\n",
    "date_sub(column, days)\t\n",
    "\n",
    "Returns the date that is `days` days after `start`\n",
    "\n",
    "88888888888888888888888888888888888888888888888888888888888888888888888888888888888888\n",
    "\n",
    "\n",
    "datediff(end, start)\t\n",
    "\n",
    "Returns the number of days from `start` to `end`.\n",
    "\n",
    "****************************************************************************************************\n",
    "months_between(end, start)\n",
    "\n",
    "Returns number of months between dates `start` and `end`.\n",
    "\n",
    "A whole number is returned if both inputs have the same day of month or both are the last day of their respective months. Otherwise, the difference is calculated assuming 31 days per month.\n",
    "***************************************************************************************************\n",
    "\n",
    "months_between(end, start, roundOff)\t\n",
    "\n",
    "Returns number of months between dates `end` and `start`. If `roundOff` is set to true, the result is rounded off to 8 digits; it is not rounded otherwise.\n",
    "\n",
    "***************************************************************************************************\n",
    "next_day(column, dayOfWeek)\t\n",
    "\n",
    "Returns the first date which is later than the value of the `date` column that is on the specified day of the week.\n",
    "\n",
    "***************************************************************************************************\n",
    "For example, `next_day('2015-07-27', \"Sunday\")` returns 2015-08-02 because that is the first Sunday after 2015-07-27.\n",
    "\n",
    "*************************************************************************************************\n",
    "\n",
    "year(column)\t\n",
    "\n",
    " year as an integer from a given date/timestamp/string\n",
    "\n",
    "******************************************************************************************\n",
    "quarter(column)   quarter as an integer from a given date/timestamp/string.\n",
    "month(column)\t  month as an integer from a given date/timestamp/string\n",
    "\n",
    "\n",
    "***********************************************************************************\n",
    "dayofweek(column)\t\n",
    "\n",
    "Extracts the day of the week as an integer \n",
    "\n",
    "Ranges from 1 for a Sunday through to 7 for a Saturday\n",
    "\n",
    "**************************************************************************************************\n",
    "dayofmonth(column)\t-- day of the month as an integer \n",
    "\n",
    "dayofyear(column)\t day of the year as an integer\n",
    "\n",
    "weekofyear(column)\t-- week number as an integer \n",
    "***************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdff17",
   "metadata": {},
   "source": [
    "# PYSPARK TIMESTAMP FUNCTION SIGNATURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ca06f",
   "metadata": {},
   "source": [
    "current_timestamp ()\t----Returns the current timestamp as a timestamp column\n",
    "\n",
    "***********************************************************************************\n",
    "hour(column)------------------ hours as an integer from a given date/timestamp/string.\n",
    "\n",
    "**************************************************************************************\n",
    "minute(column)---------------\t minutes as an integer from a given date/timestamp/string.\n",
    "\n",
    "**************************************************************************************\n",
    "second(column)---------\t seconds as an integer from a given date/timestamp/string.\n",
    "\n",
    "**********************************************************************************************\n",
    "to_timestamp(column)------\tConverts to a timestamp by casting rules to `TimestampType`.\n",
    "\n",
    "***************************************************************************************************\n",
    "to_timestamp(column, fmt)\t----Converts time string with the given pattern to timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2151f",
   "metadata": {},
   "source": [
    "# [more info datetime](https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd2383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
