{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd5d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[1]\") \\\n",
    "     .appName(\"SparkByExamples.com\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46644f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3ccf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f587af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Absenteeism_new_data.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06a4eb",
   "metadata": {},
   "source": [
    "# [PySpark When Otherwise | SQL Case When Usage](https://sparkbyexamples.com/pyspark/pyspark-when-otherwise/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b78ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a27cc312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|pets|      new|\n",
      "+----+---------+\n",
      "|   0|   nopets|\n",
      "|   4|many pets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "|   2|many pets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "+----+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+---------+\n",
      "|Pets| new_pets|\n",
      "+----+---------+\n",
      "|   0|   nopets|\n",
      "|   4|many pets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "|   2|many pets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "|   0|   nopets|\n",
      "+----+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+---------+\n",
      "|Pets|  new_pet|\n",
      "+----+---------+\n",
      "|   0|  no_pets|\n",
      "|   4|More_pets|\n",
      "|   0|  no_pets|\n",
      "|   0|  no_pets|\n",
      "|   0|  no_pets|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+---------+\n",
      "|Pets| new_pets|\n",
      "+----+---------+\n",
      "|   0|  no_pets|\n",
      "|   4|More_pets|\n",
      "|   0|  no_pets|\n",
      "|   0|  no_pets|\n",
      "|   0|  no_pets|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+---------+\n",
      "|Pets| new_pets|\n",
      "+----+---------+\n",
      "|   0|  No_pets|\n",
      "|   4|more_pets|\n",
      "|   0|  No_pets|\n",
      "|   0|  No_pets|\n",
      "|   0|  No_pets|\n",
      "|   2|more_pets|\n",
      "|   0|  No_pets|\n",
      "|   0|  No_pets|\n",
      "|   0|  No_pets|\n",
      "|   0|  No_pets|\n",
      "+----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new\",\n",
    "             when(df.Pets==0,\"nopets\").\n",
    "              when(df.Pets==1,\"only one pet\").\n",
    "              when(df.Pets>1, \"many pets\").\n",
    "              otherwise(df.Pets)\n",
    "             ).select([\"pets\",\"new\"]).show(10)\n",
    "\n",
    "df.select(df.Pets, \n",
    "          when(df.Pets==0,\"nopets\").\n",
    "              when(df.Pets==1,\"only one pet\").\n",
    "              when(df.Pets>1, \"many pets\").\n",
    "              otherwise(df.Pets).\n",
    "         alias(\"new_pets\")).show(10)\n",
    "\n",
    "\n",
    "# using sql expr function \n",
    "df.withColumn(\"new_pet\", expr(\n",
    "    \"CASE WHEN Pets = 0 THEN 'no_pets'\"+\n",
    "    \"WHEN Pets = 1 THEN 'One_pet'\"+\n",
    "    \"WHEN Pets >1 THEN 'More_pets'\"+\n",
    "    \"ELSE Pets END\"\n",
    "\n",
    ")).select([\"Pets\",\"new_pet\"]).show(5)\n",
    "\n",
    "#using sql expr with select\n",
    "\n",
    "df.select(col(\"Pets\"), expr(\n",
    "    \"CASE WHEN Pets = 0 THEN 'no_pets'\"+\n",
    "    \"WHEN Pets = 1 THEN 'One_pet'\"+\n",
    "    \"WHEN Pets >1 THEN 'More_pets'\"+\n",
    "    \"ELSE Pets END\"\n",
    "\n",
    ").alias(\"new_pets\")).select([\"Pets\",\"new_pets\"]).show(5)\n",
    "\n",
    "#using sql query with temp table\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select Pets, CASE WHEN Pets = 0 THEN 'No_pets' \" + \n",
    "               \"WHEN Pets = 1 THEN 'One_pet' WHEN Pets>1 THEN 'more_pets'\" +\n",
    "              \"ELSE Pets END as new_pets from EMP\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f774f0b",
   "metadata": {},
   "source": [
    "# [PySpark SQL expr() (Expression ) Function](https://sparkbyexamples.com/pyspark/pyspark-sql-expr-expression-function/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d7313",
   "metadata": {},
   "source": [
    " is a SQL function to execute ---\n",
    " SQL-like expressions \n",
    " \n",
    " and to use an existing DataFrame column value as an expression argument to Pyspark built-in functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e33b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given in word document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "781c50fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+\n",
      "|pets|Education|new_col|\n",
      "+----+---------+-------+\n",
      "|   0|        3|    0,3|\n",
      "|   4|        1|    4,1|\n",
      "|   0|        1|    0,1|\n",
      "|   0|        2|    0,2|\n",
      "|   0|        1|    0,1|\n",
      "+----+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concatenate two columns \n",
    "df.withColumn(\"new_col\", expr(\"Pets||','||Education\")).select([\"pets\",\"Education\",\"new_col\"]).show(5)\n",
    "\n",
    "#using sql when - refer window functions\n",
    "\n",
    "#adding incrment to date function\n",
    "\n",
    "#df.select(df.date,df.increment,\n",
    " #    expr(\"add_months(date,increment)\")\n",
    "  #.alias(\"inc_date\")).show()\n",
    "\n",
    "#artmatic func\n",
    "\n",
    "#df.select(\"Pets\",expr(\"Education+1 as new_edu\").show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ca809",
   "metadata": {},
   "source": [
    "# [PySpark lit() â€“ Add Literal or Constant to DataFrame](https://sparkbyexamples.com/pyspark/pyspark-lit-add-literal-constant/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d46c3",
   "metadata": {},
   "source": [
    "to add a new column to DataFrame by assigning a literal or constant value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d4a9c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+\n",
      "|Pets|Education|   city|\n",
      "+----+---------+-------+\n",
      "|   0|        3|newyork|\n",
      "|   4|        1|newyork|\n",
      "|   0|        1|newyork|\n",
      "|   0|        2|newyork|\n",
      "|   0|        1|newyork|\n",
      "|   2|        1|newyork|\n",
      "|   0|        1|newyork|\n",
      "|   0|        3|newyork|\n",
      "|   0|        1|newyork|\n",
      "|   0|        1|newyork|\n",
      "+----+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same value \n",
    "\n",
    "df.select(\"Pets\",\"Education\",lit(\"newyork\").alias(\"city\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97d5db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Pets| li_value|\n",
      "+----+---------+\n",
      "|   0|less_pets|\n",
      "|   4|More_pets|\n",
      "|   0|less_pets|\n",
      "|   0|less_pets|\n",
      "|   0|less_pets|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using when - otherwise \n",
    "\n",
    "df.withColumn(\"li_value\", \n",
    "              when(df.Pets<3, lit(\"less_pets\")).otherwise(lit(\"More_pets\"))).select([\"Pets\",\"li_value\"]).show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0060f",
   "metadata": {},
   "source": [
    "# [ Convert String to Array Column](https://sparkbyexamples.com/pyspark/pyspark-convert-string-to-array-column/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55065503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.select(split(col(\"name\"),\",\").alias(\"NameArray\")) \\\n",
    "#     .drop(\"name\")\n",
    "# df2.printSchema()\n",
    "# df2.show()\n",
    "\n",
    "# df.createOrReplaceTempView(\"PERSON\")\n",
    "# spark.sql(\"select SPLIT(name,',') as NameArray from PERSON\") \\\n",
    "#     .show()\n",
    "\n",
    "# splits at ,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43f5a3",
   "metadata": {},
   "source": [
    "# [Concat_ws](https://sparkbyexamples.com/pyspark/pyspark-convert-array-column-to-string-column/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35c02d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array column to a String\n",
    "\n",
    "#[jave, c, python]-- java,c,python\n",
    "\n",
    "#concat_ws(column,where to concate(,))\n",
    "\n",
    "# we can also do using sql functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb2a35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, concat_ws\n",
    "# df2 = df.withColumn(\"languagesAtSchool\",\n",
    "#    concat_ws(\",\",col(\"languagesAtSchool\")))\n",
    "\n",
    "\n",
    "# df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
    "# spark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool,\" + \\\n",
    "#     \" currentState from ARRAY_STRING\") \\\n",
    "#     .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301acfd",
   "metadata": {},
   "source": [
    "# [Get substring() from a column](https://sparkbyexamples.com/pyspark/pyspark-substring-from-a-column/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72758833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  extract the substring from a DataFrame string column\n",
    "# by providing the \n",
    "# position and length of the string \n",
    "# substring(str, pos, len)\n",
    "\n",
    "\n",
    "# df.select('date', substring('date', 1,4).alias('year'), \\\n",
    "#                   substring('date', 5,2).alias('month'), \\\n",
    "#                   substring('date', 7,2).alias('day')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\"Using with selectExpr\"\n",
    "# df2=df.selectExpr('date', 'substring(date, 1,4) as year', \\\n",
    "#                   'substring(date, 5,2) as month', \\\n",
    "#                   'substring(date, 7,2) as day')\n",
    "\n",
    "# #Using substr from Column type\n",
    "# df3=df.withColumn('year', col('date').substr(1, 4))\\\n",
    "#   .withColumn('month',col('date').substr(5, 2))\\\n",
    "#   .withColumn('day', col('date').substr(7, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d04c7",
   "metadata": {},
   "source": [
    "# [Traslate](https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/#translate-replace-character-by-character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd90eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PySpark Replace Column Values in DataFrame\n",
    "#regex_replace \n",
    "\n",
    "# df.withColumn('address', regexp_replace('address', 'Rd', 'Road')) \\\n",
    "#   .show(truncate=False)\n",
    "\n",
    "print(\"*****************************************************************************************\")\n",
    "\n",
    "#Replace values from Dictionary\n",
    "\n",
    "# stateDic={'CA':'California','NY':'New York','DE':'Delaware'}\n",
    "# df2=df.rdd.map(lambda x: \n",
    "#     (x.id,x.address,stateDic[x.state]) \n",
    "#     ).toDF([\"id\",\"address\",\"state\"])\n",
    "# df2.show()\n",
    "\n",
    "print(\"********************************************************************************************\")\n",
    "\n",
    "#Using translate to replace character by character\n",
    "\n",
    "# from pyspark.sql.functions import translate\n",
    "# df.withColumn('address', translate('address', '123', 'ABC')) \\\n",
    "#   .show(truncate=False)\n",
    "\n",
    "print('********************************************************************************************')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
